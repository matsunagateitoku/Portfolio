{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ziJbyckx-UPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "New test cell"
      ],
      "metadata": {
        "id": "hKW4Pa3--YHE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ro15rcE-SgY"
      },
      "outputs": [],
      "source": [
        "# Text preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yeLeHVS-Sgc"
      },
      "outputs": [],
      "source": [
        "%reset -f\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import nltk, pandas as pd, numpy.testing as npt, unicodedata, contractions, re\n",
        "from numpy.testing import assert_equal as eq\n",
        "import unittest\n",
        "from colorunittest import run_unittest\n",
        "_ = nltk.download(['omw-1.4','brown','wordnet','stopwords','averaged_perceptron_tagger'], quiet=True)\n",
        "from nltk.corpus import brown, stopwords\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ\n",
        "\n",
        "class Pipe():\n",
        "    ### TASK 1: Attribute Initialization\n",
        "    def __init__(self, LsWords=[], SsLex=set(), SsStopWords=set()) -> object:\n",
        "        assert isinstance(LsWords, list) or LsWords is None, f'LsWords must be a list, not a {type(LsWords)}'\n",
        "        assert isinstance(SsLex, set) or (SsLex=='nltk'), f'SsLex must be \"nltk\" or a set of lexicon words, not a {type(SsLex)}'\n",
        "        assert isinstance(SsStopWords, set) or (SsStopWords=='nltk'), f'SsStopWords must be \"nltk\" or a set of words, not a {type(SsStopWords)}'\n",
        "        self.df = pd.DataFrame(columns = ['Step', 'Words', 'Vocab', 'CorrVocab'])\n",
        "\n",
        "        _ = nltk.download(['brown'], quiet=True)\n",
        "        Ss6 = {s.lower() for s in nltk.corpus.brown.words()}\n",
        "\n",
        "        self.LsWords = LsWords\n",
        "        if SsLex =='nltk':\n",
        "            self.SsLex = Ss6\n",
        "        else:\n",
        "            self.SsLex = SsLex\n",
        "        if SsStopWords =='nltk':\n",
        "            self.SsStopWords = set(stopwords.words('english'))\n",
        "        else:\n",
        "            self.SsStopWords = SsStopWords\n",
        "\n",
        "        self.AddStats('Initialize')     # Saves basic stats for LsWord\n",
        "\n",
        "    ### TASK 2: Output\n",
        "    def Out(self) -> str:\n",
        "        cleaned_string = ' '.join(self.LsWords)\n",
        "        cleaned_string = re.sub(r'\\s+', ' ', cleaned_string)\n",
        "        return cleaned_string.strip()\n",
        "\n",
        "    ### TASK 3: Lowercase\n",
        "    @property\n",
        "    def Low(self) -> object:\n",
        "        self.LsWords = [w.lower() for w in self.LsWords]  # Lowercase each word token\n",
        "        self.AddStats('Lower')  # Update statistics for this step **** not sure about this\n",
        "        return self  # Return reference to self for method chaining\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 4: Remove Digits\n",
        "    @property\n",
        "    def NoNum(self) -> object:\n",
        "        self.LsWords = [re.sub(r'\\d+', '', word) for word in self.LsWords]  # Remove all digits\n",
        "        self.AddStats('NoNum')  # Update statistics for this step\n",
        "        return self\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 5: Keep Only Word Characters\n",
        "    @property\n",
        "    def Words(self) -> object:\n",
        "        self.LsWords = [re.sub(r'[^\\w\\s]+', '', word) for word in self.LsWords]\n",
        "        self.AddStats('Words')\n",
        "        return self\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 6: Remove Stop Words\n",
        "    @property\n",
        "    def Stop(self) -> object:\n",
        "        self.LsWords = [word for word in self.LsWords if word.lower() not in self.SsStopWords]\n",
        "        self.AddStats('Stop')\n",
        "        return self\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 7: Normalization\n",
        "    @property\n",
        "    def Norm(self) -> object:\n",
        "        self.LsWords = [unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8') for word in self.LsWords]\n",
        "        self.AddStats('Norm')  # Update statistics for this step\n",
        "        return self  # Return reference to self for method chaining\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 8: Expand Contractions\n",
        "    @property\n",
        "    def Exp(self) -> object:\n",
        "        concatenated_string = ' '.join(self.LsWords)\n",
        "        expanded_string = contractions.fix(concatenated_string)\n",
        "        self.LsWords = expanded_string.split()\n",
        "        self.AddStats('Exp')  # Update statistics for this step\n",
        "        return self  # Return reference to self for method chaining\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "    ### TASK 9: Stem\n",
        "    @property\n",
        "    def Stem(self) -> object:\n",
        "        pso = nltk.stem.PorterStemmer()       # instantiates Porter Stemmer object\n",
        "        self.LsWords = [pso.stem(word) for word in self.LsWords]\n",
        "        self.AddStats('Stem')\n",
        "        return self\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    ### TASK 10: Lemmatize\n",
        "    @property\n",
        "    def Lem(self) -> object:\n",
        "        wlo = nltk.stem.WordNetLemmatizer()   # instantiates WordNet Lemmatizer object\n",
        "        WNTag = lambda t: t[0].lower() if t[0] in 'ARNV' else 'n'   # Converts NLTK POS Tag to WordNet POS Tag\n",
        "        # Create a list of tuples of words & their WordNet POS tags,\n",
        "        #    i.e. 'a' for adjectives, 'r' for adverbs, 'v' for verbs, 'n' for nouns and all else\n",
        "        LTssWordTag = [(word, WNTag(tag)) for word, tag in nltk.pos_tag(self.LsWords)]\n",
        "        WNTag = lambda t: t[0].lower() if t[0] in 'ARNV' else 'n'\n",
        "        LTssWordTag = [(word, WNTag(tag)) for word, tag in nltk.pos_tag(self.LsWords)]\n",
        "        self.LsWords = [wlo.lemmatize(word, tag) for word, tag in LTssWordTag]\n",
        "        self.AddStats('Lem')  # Update statistics for this step\n",
        "        return self\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def AddStats(self, sTask='') -> object:\n",
        "        SsWords = {s for s in self.LsWords}\n",
        "        self.df.loc[len(self.df)] = [sTask, len(self.LsWords), len(SsWords), len(SsWords.intersection(self.SsLex))]\n",
        "        return self     # Finally, return reference to the object itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLH7xhRA-Sge",
        "outputId": "50e69344-49ec-4ea5-9563-14b7028bad97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'stori tell children sara cone bryant two littl riddl rhyme garden ken full littl gentlemen littl cap blue wear green ribbon veri fair flax hous hous goe messeng small slight whether rain snow sleep outsid night path littl yellow tulip onc wa littl yellow tulip live littl dark hous ground one day wa sit wa veri still suddenli heard littl _tap tap tap_ door said rain want come said soft sad littl voic come littl tulip said heard anoth littl _tap tap tap_ window pane said soft littl voic answer rai'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Step</th>\n",
              "      <th>Words</th>\n",
              "      <th>Vocab</th>\n",
              "      <th>CorrVocab</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Initialize</td>\n",
              "      <td>55563</td>\n",
              "      <td>4420</td>\n",
              "      <td>3307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Lower</td>\n",
              "      <td>55563</td>\n",
              "      <td>3940</td>\n",
              "      <td>3448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Norm</td>\n",
              "      <td>55563</td>\n",
              "      <td>3940</td>\n",
              "      <td>3448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Exp</td>\n",
              "      <td>55572</td>\n",
              "      <td>3935</td>\n",
              "      <td>3445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Words</td>\n",
              "      <td>55572</td>\n",
              "      <td>3896</td>\n",
              "      <td>3432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Stem</td>\n",
              "      <td>55572</td>\n",
              "      <td>2998</td>\n",
              "      <td>1955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Stop</td>\n",
              "      <td>32561</td>\n",
              "      <td>2882</td>\n",
              "      <td>1848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>NoNum</td>\n",
              "      <td>32561</td>\n",
              "      <td>2880</td>\n",
              "      <td>1846</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Step  Words Vocab CorrVocab\n",
              "0  Initialize  55563  4420      3307\n",
              "1       Lower  55563  3940      3448\n",
              "2        Norm  55563  3940      3448\n",
              "3         Exp  55572  3935      3445\n",
              "4       Words  55572  3896      3432\n",
              "5        Stem  55572  2998      1955\n",
              "6        Stop  32561  2882      1848\n",
              "7       NoNum  32561  2880      1846"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_ = nltk.download(['gutenberg'], quiet=True)\n",
        "LsBookWords = list(nltk.corpus.gutenberg.words('bryant-stories.txt')) #[:1000]\n",
        "sSampleText = nltk.corpus.gutenberg.raw('bryant-stories.txt')[:500] + '...\\n'\n",
        "\n",
        "pp = Pipe(LsBookWords, SsStopWords='nltk', SsLex='nltk').Low.Norm.Exp.Words.Stem.Stop.NoNum\n",
        "pp.Out()[:500]\n",
        "pp.df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z_36ozP-Sgf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYNPteFE-Sgf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHGFkJTG-Sgg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
